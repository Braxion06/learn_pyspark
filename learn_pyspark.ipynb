{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d7fef7",
   "metadata": {},
   "source": [
    "# Learn and practice pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ac02d5",
   "metadata": {},
   "source": [
    "## Prepare the  environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27771ef9",
   "metadata": {},
   "source": [
    "Import necessary modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b34bd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType\n",
    "# from pyspark.sql import functions as sf\n",
    "from pyspark.sql.functions import col, concat, lit, when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58f37c",
   "metadata": {},
   "source": [
    "Prepare the project's directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa52bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare directories\n",
    "os.makedirs(\"data/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860185e1",
   "metadata": {},
   "source": [
    "Download a subset of the NYC taxi trips dataset. This subset contains the trips of yellow taxis from march 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40ca963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready\n"
     ]
    }
   ],
   "source": [
    "# Download NYC taxi trips dataset\n",
    "URL = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-03.parquet\"\n",
    "response = requests.get(url=URL, stream=True, timeout=(10, 30))\n",
    "\n",
    "if os.path.exists(\"data/yellow_taxi_data_202503.parquet\"):\n",
    "    print(\"Dataset ready\")\n",
    "else:\n",
    "    with open(\"data/yellow_taxi_data_202503.parquet\", \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=4092):\n",
    "            file.write(chunk)\n",
    "    print(\"Dataset downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538bbd2",
   "metadata": {},
   "source": [
    "# Pyspark basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8048a6",
   "metadata": {},
   "source": [
    "Initialize Spark Session, this is the basic entrypoint to interact with Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce438ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 20:57:53 WARN Utils: Your hostname, xblade resolves to a loopback address: 127.0.1.1; using 192.168.10.156 instead (on interface enp2s0)\n",
      "25/07/09 20:57:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/09 20:57:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"LearnSpark\") \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.sql.shuffle.partitions', '200') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad03fb57",
   "metadata": {},
   "source": [
    "In the SparkSession builder we can configure the spark deployment and its process like the amount of memory and cores assigned to the executor process. The [configuration docs are Here](!https://spark.apache.org/docs/latest/configuration.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e024d56",
   "metadata": {},
   "source": [
    "## The Dataframe basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6bb5b",
   "metadata": {},
   "source": [
    "Create a Dataframe from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e954c127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Name: string, Age: bigint]\n"
     ]
    }
   ],
   "source": [
    "df_plain = spark.createDataFrame(data=[('Alan', 10), ('Cindy', 12)], schema=('Name', 'Age'))\n",
    "print(df_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a47ea7",
   "metadata": {},
   "source": [
    "Create a Dataframe by reading a file, it could be parquet, json, csv, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e1abcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = spark.read.parquet(\"data/yellow_taxi_data_202503.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542a3ec8",
   "metadata": {},
   "source": [
    "Show the first rows of a dataframe with Dataframe.show(), the default value is 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e0103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       1| 2025-03-01 00:17:16|  2025-03-01 00:25:52|              1|          0.9|         1|                 N|         140|         236|           1|        7.9|  3.5|    0.5|       2.6|         0.0|                  1.0|        15.5|                 2.5|        0.0|               0.0|\n",
      "|       1| 2025-03-01 00:37:38|  2025-03-01 00:43:51|              1|          0.6|         1|                 N|         140|         262|           1|        6.5|  3.5|    0.5|       2.3|         0.0|                  1.0|        13.8|                 2.5|        0.0|               0.0|\n",
      "|       2| 2025-03-01 00:24:35|  2025-03-01 00:39:49|              1|         1.94|         1|                 N|         161|          68|           1|       14.9|  1.0|    0.5|      5.16|         0.0|                  1.0|       25.81|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:56:16|  2025-03-01 01:01:35|              2|         0.95|         1|                 N|         231|          13|           1|        7.2|  1.0|    0.5|      2.59|         0.0|                  1.0|       15.54|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:01:44|  2025-03-01 00:10:00|              1|          1.5|         1|                 N|         163|         236|           1|        8.6| 4.25|    0.5|      2.85|         0.0|                  1.0|        17.2|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:11:57|  2025-03-01 00:28:33|              0|          2.0|         1|                 N|         166|          74|           1|       16.3|  1.0|    0.5|       2.0|         0.0|                  1.0|        20.8|                 0.0|        0.0|               0.0|\n",
      "|       2| 2025-03-01 00:22:35|  2025-03-01 00:34:06|              2|         3.27|         1|                 N|          88|          79|           1|       17.0|  1.0|    0.5|      4.55|         0.0|                  1.0|        27.3|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:37:22|  2025-03-01 00:45:03|              1|         0.95|         1|                 N|         114|         107|           1|        8.6|  1.0|    0.5|       2.0|         0.0|                  1.0|       16.35|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-02-28 23:50:41|  2025-03-01 00:03:51|              1|         2.09|         1|                 N|          79|         186|           1|       13.5|  1.0|    0.5|      3.85|         0.0|                  1.0|        23.1|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:06:48|  2025-03-01 00:18:44|              1|         1.43|         1|                 N|         186|         107|           1|       12.1|  1.0|    0.5|      3.57|         0.0|                  1.0|       21.42|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:27:25|  2025-03-01 00:34:41|              1|         0.89|         1|                 N|          79|         211|           1|        8.6|  1.0|    0.5|       0.0|         0.0|                  1.0|       14.35|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:39:38|  2025-03-01 00:43:49|              1|         0.72|         1|                 N|         211|         231|           1|        6.5|  1.0|    0.5|       2.0|         0.0|                  1.0|       14.25|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:44:28|  2025-03-01 01:25:34|              2|         18.6|         2|                 N|         132|          48|           2|       70.0|  5.0|    0.5|       0.0|        6.94|                  1.0|       83.44|                 2.5|       1.75|              0.75|\n",
      "|       2| 2025-03-01 00:00:15|  2025-03-01 01:03:30|              4|        23.28|         1|                 N|         132|         220|           2|      101.7|  1.0|    0.5|       0.0|        6.94|                  1.0|      112.89|                 0.0|       1.75|               0.0|\n",
      "|       2| 2025-03-01 00:02:02|  2025-03-01 00:18:45|              1|         2.73|         1|                 N|         163|         263|           1|       17.7|  1.0|    0.5|      4.69|         0.0|                  1.0|       28.14|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:21:17|  2025-03-01 00:25:33|              1|         1.29|         1|                 N|         141|         229|           1|        7.9|  1.0|    0.5|      2.05|         0.0|                  1.0|        15.7|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:41:51|  2025-03-01 00:50:56|              1|         1.99|         1|                 N|         211|         137|           1|       11.4|  1.0|    0.5|       1.0|         0.0|                  1.0|       18.15|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:29:44|  2025-03-01 00:35:25|              1|          0.9|         1|                 N|         246|          90|           1|        7.9| 4.25|    0.5|       2.7|         0.0|                  1.0|       16.35|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:41:30|  2025-03-01 00:53:25|              2|          2.4|         1|                 N|         249|         163|           1|       14.9| 4.25|    0.5|       4.1|         0.0|                  1.0|       24.75|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:06:21|  2025-03-01 00:12:06|              2|         0.59|         1|                 N|         186|          68|           2|        7.2|  1.0|    0.5|       0.0|         0.0|                  1.0|       12.95|                 2.5|        0.0|              0.75|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3be5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       1| 2025-03-01 00:17:16|  2025-03-01 00:25:52|              1|          0.9|         1|                 N|         140|         236|           1|        7.9|  3.5|    0.5|       2.6|         0.0|                  1.0|        15.5|                 2.5|        0.0|               0.0|\n",
      "|       1| 2025-03-01 00:37:38|  2025-03-01 00:43:51|              1|          0.6|         1|                 N|         140|         262|           1|        6.5|  3.5|    0.5|       2.3|         0.0|                  1.0|        13.8|                 2.5|        0.0|               0.0|\n",
      "|       2| 2025-03-01 00:24:35|  2025-03-01 00:39:49|              1|         1.94|         1|                 N|         161|          68|           1|       14.9|  1.0|    0.5|      5.16|         0.0|                  1.0|       25.81|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:56:16|  2025-03-01 01:01:35|              2|         0.95|         1|                 N|         231|          13|           1|        7.2|  1.0|    0.5|      2.59|         0.0|                  1.0|       15.54|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:01:44|  2025-03-01 00:10:00|              1|          1.5|         1|                 N|         163|         236|           1|        8.6| 4.25|    0.5|      2.85|         0.0|                  1.0|        17.2|                 2.5|        0.0|              0.75|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# only the first 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2dc77c",
   "metadata": {},
   "source": [
    "The Dataframe.head() Returns the first n rows like Pandas (1 by default on spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b6d26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 17, 16), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 25, 52), passenger_count=1, trip_distance=0.9, RatecodeID=1, store_and_fwd_flag='N', PULocationID=140, DOLocationID=236, payment_type=1, fare_amount=7.9, extra=3.5, mta_tax=0.5, tip_amount=2.6, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=15.5, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 37, 38), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 43, 51), passenger_count=1, trip_distance=0.6, RatecodeID=1, store_and_fwd_flag='N', PULocationID=140, DOLocationID=262, payment_type=1, fare_amount=6.5, extra=3.5, mta_tax=0.5, tip_amount=2.3, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=13.8, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 24, 35), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 39, 49), passenger_count=1, trip_distance=1.94, RatecodeID=1, store_and_fwd_flag='N', PULocationID=161, DOLocationID=68, payment_type=1, fare_amount=14.9, extra=1.0, mta_tax=0.5, tip_amount=5.16, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=25.81, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 56, 16), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 1, 1, 35), passenger_count=2, trip_distance=0.95, RatecodeID=1, store_and_fwd_flag='N', PULocationID=231, DOLocationID=13, payment_type=1, fare_amount=7.2, extra=1.0, mta_tax=0.5, tip_amount=2.59, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=15.54, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 1, 44), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 10), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag='N', PULocationID=163, DOLocationID=236, payment_type=1, fare_amount=8.6, extra=4.25, mta_tax=0.5, tip_amount=2.85, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=17.2, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55779de0",
   "metadata": {},
   "source": [
    "The DataFrame.take() method works similarly to head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170fc189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 17, 16), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 25, 52), passenger_count=1, trip_distance=0.9, RatecodeID=1, store_and_fwd_flag='N', PULocationID=140, DOLocationID=236, payment_type=1, fare_amount=7.9, extra=3.5, mta_tax=0.5, tip_amount=2.6, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=15.5, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 37, 38), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 43, 51), passenger_count=1, trip_distance=0.6, RatecodeID=1, store_and_fwd_flag='N', PULocationID=140, DOLocationID=262, payment_type=1, fare_amount=6.5, extra=3.5, mta_tax=0.5, tip_amount=2.3, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=13.8, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.0),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 24, 35), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 39, 49), passenger_count=1, trip_distance=1.94, RatecodeID=1, store_and_fwd_flag='N', PULocationID=161, DOLocationID=68, payment_type=1, fare_amount=14.9, extra=1.0, mta_tax=0.5, tip_amount=5.16, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=25.81, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 56, 16), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 1, 1, 35), passenger_count=2, trip_distance=0.95, RatecodeID=1, store_and_fwd_flag='N', PULocationID=231, DOLocationID=13, payment_type=1, fare_amount=7.2, extra=1.0, mta_tax=0.5, tip_amount=2.59, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=15.54, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2025, 3, 1, 0, 1, 44), tpep_dropoff_datetime=datetime.datetime(2025, 3, 1, 0, 10), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag='N', PULocationID=163, DOLocationID=236, payment_type=1, fare_amount=8.6, extra=4.25, mta_tax=0.5, tip_amount=2.85, tolls_amount=0.0, improvement_surcharge=1.0, total_amount=17.2, congestion_surcharge=2.5, Airport_fee=0.0, cbd_congestion_fee=0.75)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5265ef1",
   "metadata": {},
   "source": [
    "We can create a new Pandas Dataframe based on the original dataframe with Dataframe.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e1ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pd_df = df_plain.toPandas()\n",
    "print(type(pd_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0984cc",
   "metadata": {},
   "source": [
    "Show the DataFrame  with Pandas's Dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5abcc94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cindy</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Name  Age\n",
       "0   Alan   10\n",
       "1  Cindy   12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9aef9",
   "metadata": {},
   "source": [
    "Write a dataframe to csv files.\n",
    "The write.csv() method will create a directory and write multiple csv files(parts of the dataframe) that together, make up the whole dataframe and _SUCCESS files as confirmation.\n",
    "\n",
    "The .csv() method should be called after .option() and .mode() methods. In this case the header = true is used to write a header with the column names in the .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33ec944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.option('header', 'true').mode('overwrite').csv('data/yellow_taxi_data_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea083aa8",
   "metadata": {},
   "source": [
    "The df.write() method returns a **DataFrameWriter** object that has many modes:\n",
    "\n",
    "**append**: Append contents of this DataFrame to existing data.\n",
    "\n",
    "**overwrite**: Overwrite existing data.\n",
    "\n",
    "**error or errorifexists**: Throw an exception if data already exists.\n",
    "\n",
    "**ignore**: Silently ignore this operation if data already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a53d1e",
   "metadata": {},
   "source": [
    "Read the csv data as a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "569c3ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+------------------+\n",
      "|     _c0|                 _c1|                 _c2|            _c3|          _c4|       _c5|               _c6|         _c7|         _c8|         _c9|       _c10| _c11|   _c12|      _c13|        _c14|                _c15|        _c16|                _c17|       _c18|              _c19|\n",
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_date...|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surch...|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "|       1|2025-03-11T11:00:...|2025-03-11T11:13:...|              2|          1.6|         1|                 Y|         170|         163|           1|       13.5| 3.25|    0.5|       2.0|         0.0|                 1.0|       20.25|                 2.5|        0.0|              0.75|\n",
      "|       1|2025-03-11T11:15:...|2025-03-11T11:28:...|              2|          1.5|         1|                 N|         163|         236|           1|       12.8| 3.25|    0.5|       2.0|         0.0|                 1.0|       19.55|                 2.5|        0.0|              0.75|\n",
      "|       1|2025-03-11T11:58:...|2025-03-11T12:13:...|              1|          1.8|         1|                 N|         170|         237|           1|       14.2| 3.25|    0.5|       2.5|         0.0|                 1.0|       21.45|                 2.5|        0.0|              0.75|\n",
      "|       1|2025-03-11T11:04:...|2025-03-11T11:09:...|              1|          0.7|         1|                 N|         170|         161|           1|        6.5| 3.25|    0.5|       2.8|         0.0|                 1.0|       14.05|                 2.5|        0.0|              0.75|\n",
      "+--------+--------------------+--------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+--------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.read.csv('data/yellow_taxi_data_csv')\n",
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4837319c",
   "metadata": {},
   "source": [
    "The dataframe columns were not inferred, the header = true option must be called. The option inferSchema is useful too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dfa9e",
   "metadata": {},
   "source": [
    "First let's check the current schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28c4df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      " |-- _c12: string (nullable = true)\n",
      " |-- _c13: string (nullable = true)\n",
      " |-- _c14: string (nullable = true)\n",
      " |-- _c15: string (nullable = true)\n",
      " |-- _c16: string (nullable = true)\n",
      " |-- _c17: string (nullable = true)\n",
      " |-- _c18: string (nullable = true)\n",
      " |-- _c19: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0f5a1",
   "metadata": {},
   "source": [
    "Now let's read again it using both options, passed as a dictionary with the **.options** method, this is different from **.option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c998524",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# we can also use the read.csv() method and pass parameter to it\n",
    "# df_csv = spark.read.csv('data/yellow_taxi_data_csv', header=true, inferschema=true)\n",
    "df_csv = spark.read.options(header=True, inferschema=True, delimiter=',').csv('data/yellow_taxi_data_csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237720e",
   "metadata": {},
   "source": [
    "The columns's names are mapped now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16f73325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       1| 2025-03-11 11:00:41|  2025-03-11 11:13:49|              2|          1.6|         1|                 Y|         170|         163|           1|       13.5| 3.25|    0.5|       2.0|         0.0|                  1.0|       20.25|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-11 11:15:56|  2025-03-11 11:28:01|              2|          1.5|         1|                 N|         163|         236|           1|       12.8| 3.25|    0.5|       2.0|         0.0|                  1.0|       19.55|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-11 11:58:39|  2025-03-11 12:13:17|              1|          1.8|         1|                 N|         170|         237|           1|       14.2| 3.25|    0.5|       2.5|         0.0|                  1.0|       21.45|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-11 11:04:05|  2025-03-11 11:09:28|              1|          0.7|         1|                 N|         170|         161|           1|        6.5| 3.25|    0.5|       2.8|         0.0|                  1.0|       14.05|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-11 11:14:26|  2025-03-11 11:27:28|              1|          2.0|         1|                 N|         237|         239|           1|       12.8|  2.5|    0.5|       2.2|         0.0|                  1.0|        19.0|                 2.5|        0.0|               0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f78c2e",
   "metadata": {},
   "source": [
    "And the dataframe has a different(and much better) schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b2f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa6c0ed",
   "metadata": {},
   "source": [
    "The dataframe.schema Attribute holds the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d39cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampType(), True), StructField('tpep_dropoff_datetime', TimestampType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', IntegerType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True), StructField('cbd_congestion_fee', DoubleType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df_csv.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1dd39",
   "metadata": {},
   "source": [
    "We can create a schema definition using the StructType and StructField classes, the structfield constructor parameters are: StructField(name, dataType, nullable=True, metadata=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d54ebf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe schema:\n",
      "StructType([StructField('VendorID', IntegerType(), True), StructField('tpep_pickup_datetime', TimestampType(), True), StructField('tpep_dropoff_datetime', TimestampType(), True), StructField('passenger_count', IntegerType(), True), StructField('trip_distance', DoubleType(), True), StructField('RatecodeID', IntegerType(), True), StructField('store_and_fwd_flag', StringType(), True), StructField('PULocationID', IntegerType(), True), StructField('DOLocationID', IntegerType(), True), StructField('payment_type', IntegerType(), True), StructField('fare_amount', DoubleType(), True), StructField('extra', DoubleType(), True), StructField('mta_tax', DoubleType(), True), StructField('tip_amount', DoubleType(), True), StructField('tolls_amount', DoubleType(), True), StructField('improvement_surcharge', DoubleType(), True), StructField('total_amount', DoubleType(), True), StructField('congestion_surcharge', DoubleType(), True), StructField('Airport_fee', DoubleType(), True), StructField('cbd_congestion_fee', DoubleType(), True)])\n"
     ]
    }
   ],
   "source": [
    "df_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"Airport_fee\", DoubleType(), True),\n",
    "    StructField(\"cbd_congestion_fee\", DoubleType(), True)\n",
    "])\n",
    "print(f'Dataframe schema:\\n{df_schema}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d152896",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.schema(schema=df_schema).csv('data/yellow_taxi_data_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35f25891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      " |-- cbd_congestion_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7b5be",
   "metadata": {},
   "source": [
    "We can print only the columns of the dataframe with the Dataframe.columns Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "102a5df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35992fb1",
   "metadata": {},
   "source": [
    "The DataFrame.dtypes Attribute can be useful if we only want the columans and their types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe98b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('VendorID', 'int'), ('tpep_pickup_datetime', 'timestamp_ntz'), ('tpep_dropoff_datetime', 'timestamp_ntz'), ('passenger_count', 'bigint'), ('trip_distance', 'double'), ('RatecodeID', 'bigint'), ('store_and_fwd_flag', 'string'), ('PULocationID', 'int'), ('DOLocationID', 'int'), ('payment_type', 'bigint'), ('fare_amount', 'double'), ('extra', 'double'), ('mta_tax', 'double'), ('tip_amount', 'double'), ('tolls_amount', 'double'), ('improvement_surcharge', 'double'), ('total_amount', 'double'), ('congestion_surcharge', 'double'), ('Airport_fee', 'double'), ('cbd_congestion_fee', 'double')]\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d870f930",
   "metadata": {},
   "source": [
    "With a correct schema, we can use the DataFrame.summary() method to generate a new dataframe with the statistics of said DataFrame. Let's use the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "020d8325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/09 20:58:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+------------------+\n",
      "|summary|          VendorID|   passenger_count|    trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|      payment_type|       fare_amount|             extra|           mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|        Airport_fee|cbd_congestion_fee|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+------------------+\n",
      "|  count|           4145257|           3228594|          4145257|           3228594|           3228594|           4145257|          4145257|           4145257|           4145257|           4145257|           4145257|           4145257|           4145257|              4145257|           4145257|             3228594|            3228594|           4145257|\n",
      "|   mean|1.8249437851501125|1.2911199116395558|6.584099784403299| 2.423492393283268|              NULL|161.76531346548597|161.2287341894604|0.9560951226908248|17.800342562597166|1.2209241091686227|0.4785716711895065| 2.858856020265468|0.4749764924103659|    0.957108570107958|26.265898046844185|  2.2193944175080547|0.13512275002679186|0.5364268126198207|\n",
      "| stddev|0.5490765912853165|0.7317499817376729| 626.407536754718|11.360100348918557|              NULL|  65.9219960487389| 70.1327954566159|0.7362719752636325| 29.10144097036428|1.8501915009119179|0.1365122801872657|3.8448975263552603|2.1027958622778935|   0.2721862120192103|  31.9560839230314|  0.9155161802746349|0.49965083191910353|0.3569130718580522|\n",
      "|    min|                 1|                 0|              0.0|                 1|                 N|                 1|                1|                 0|            -999.0|             -9.25|              -0.5|            -92.09|           -142.17|                 -1.0|           -1000.0|                -2.5|              -1.75|             -0.75|\n",
      "|    25%|                 2|                 1|             1.03|                 1|              NULL|               114|              107|                 1|               8.6|               0.0|               0.5|               0.0|               0.0|                  1.0|             15.66|                 2.5|                0.0|               0.0|\n",
      "|    50%|                 2|                 1|              1.8|                 1|              NULL|               161|              162|                 1|              13.5|               0.0|               0.5|              2.17|               0.0|                  1.0|             20.75|                 2.5|                0.0|              0.75|\n",
      "|    75%|                 2|                 1|             3.42|                 1|              NULL|               232|              233|                 1|             21.26|               2.5|               0.5|              3.93|               0.0|                  1.0|             29.45|                 2.5|                0.0|              0.75|\n",
      "|    max|                 7|                 9|        320136.29|                99|                 Y|               265|              265|                 4|          46263.88|              13.5|              10.5|             290.0|            916.87|                  1.0|          46269.44|                 2.5|               6.75|               1.5|\n",
      "+-------+------------------+------------------+-----------------+------------------+------------------+------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+---------------------+------------------+--------------------+-------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc57d424",
   "metadata": {},
   "source": [
    "If we only want the count here's the simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a029b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 4145257\n"
     ]
    }
   ],
   "source": [
    "print(f'Count: {df.count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2e197c",
   "metadata": {},
   "source": [
    "# Dataframe Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303910c8",
   "metadata": {},
   "source": [
    "## Select operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a81bc5",
   "metadata": {},
   "source": [
    "Select: Returns a dataframe with only the **selected** columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d003497a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-----+----------+------------+\n",
      "|VendorID|fare_amount|extra|tip_amount|total_amount|\n",
      "+--------+-----------+-----+----------+------------+\n",
      "|       1|        7.9|  3.5|       2.6|        15.5|\n",
      "|       1|        6.5|  3.5|       2.3|        13.8|\n",
      "|       2|       14.9|  1.0|      5.16|       25.81|\n",
      "|       2|        7.2|  1.0|      2.59|       15.54|\n",
      "|       1|        8.6| 4.25|      2.85|        17.2|\n",
      "+--------+-----------+-----+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'fare_amount', 'extra', 'tip_amount', 'total_amount').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8f498",
   "metadata": {},
   "source": [
    "We  can use SQL `*` in a select method just like SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "792ab056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|cbd_congestion_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "|       1| 2025-03-01 00:17:16|  2025-03-01 00:25:52|              1|          0.9|         1|                 N|         140|         236|           1|        7.9|  3.5|    0.5|       2.6|         0.0|                  1.0|        15.5|                 2.5|        0.0|               0.0|\n",
      "|       1| 2025-03-01 00:37:38|  2025-03-01 00:43:51|              1|          0.6|         1|                 N|         140|         262|           1|        6.5|  3.5|    0.5|       2.3|         0.0|                  1.0|        13.8|                 2.5|        0.0|               0.0|\n",
      "|       2| 2025-03-01 00:24:35|  2025-03-01 00:39:49|              1|         1.94|         1|                 N|         161|          68|           1|       14.9|  1.0|    0.5|      5.16|         0.0|                  1.0|       25.81|                 2.5|        0.0|              0.75|\n",
      "|       2| 2025-03-01 00:56:16|  2025-03-01 01:01:35|              2|         0.95|         1|                 N|         231|          13|           1|        7.2|  1.0|    0.5|      2.59|         0.0|                  1.0|       15.54|                 2.5|        0.0|              0.75|\n",
      "|       1| 2025-03-01 00:01:44|  2025-03-01 00:10:00|              1|          1.5|         1|                 N|         163|         236|           1|        8.6| 4.25|    0.5|      2.85|         0.0|                  1.0|        17.2|                 2.5|        0.0|              0.75|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('*').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32a4a2a",
   "metadata": {},
   "source": [
    "We can select columns with Pandas's style too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0ae98cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       2|       25.81|\n",
      "|       2|       15.54|\n",
      "|       1|        17.2|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'total_amount'].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703a696",
   "metadata": {},
   "source": [
    "Or an Attribute style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67e5f188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       2|       25.81|\n",
      "|       2|       15.54|\n",
      "|       1|        17.2|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8f2c8",
   "metadata": {},
   "source": [
    "Dataframe.limit(): Limits the result count of a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34cd006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       2|       25.81|\n",
      "|       2|       15.54|\n",
      "|       1|        17.2|\n",
      "|       1|        20.8|\n",
      "|       2|        27.3|\n",
      "|       2|       16.35|\n",
      "|       2|        23.1|\n",
      "|       2|       21.42|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).limit(10).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd682d7f",
   "metadata": {},
   "source": [
    "DataFrame.collect() returns all the rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8339a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=1, total_amount=15.5),\n",
       " Row(VendorID=1, total_amount=13.8),\n",
       " Row(VendorID=2, total_amount=25.81),\n",
       " Row(VendorID=2, total_amount=15.54),\n",
       " Row(VendorID=1, total_amount=17.2),\n",
       " Row(VendorID=1, total_amount=20.8),\n",
       " Row(VendorID=2, total_amount=27.3),\n",
       " Row(VendorID=2, total_amount=16.35),\n",
       " Row(VendorID=2, total_amount=23.1),\n",
       " Row(VendorID=2, total_amount=21.42)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).limit(10).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bd4b0",
   "metadata": {},
   "source": [
    "We can return a column using the col() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "598f8f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       2|       25.81|\n",
      "|       2|       15.54|\n",
      "|       1|        17.2|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(col='VendorID'), col('total_amount')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940906ab",
   "metadata": {},
   "source": [
    "Dataframe Aliases (another name like in sql), they are useful to refer a column within a dataframe in joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "acb89e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_as = df.alias('nyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e190304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       2|       25.81|\n",
      "|       2|       15.54|\n",
      "|       1|        17.2|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_as.select(col('nyc.VendorID'), col('nyc.total_amount')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b797912",
   "metadata": {},
   "source": [
    "Columns Aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "35fbec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|VendorID|TOTAL_ALIAS|\n",
      "+--------+-----------+\n",
      "|       1|       15.5|\n",
      "|       1|       13.8|\n",
      "|       2|      25.81|\n",
      "|       2|      15.54|\n",
      "|       1|       17.2|\n",
      "+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', col('total_amount').alias('TOTAL_ALIAS')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdfa3dc",
   "metadata": {},
   "source": [
    "Sorting Data:\n",
    "\n",
    "We can sort the data with the .sort() method, any number of columns can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b652bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|VendorID|TOTAL_ALIAS|\n",
      "+--------+-----------+\n",
      "|       1|   46269.44|\n",
      "|       2|    2246.55|\n",
      "|       2|    1694.31|\n",
      "|       2|     1249.3|\n",
      "|       2|    1240.52|\n",
      "|       2|    1226.25|\n",
      "|       2|     1189.2|\n",
      "|       2|    1183.39|\n",
      "|       2|    1175.52|\n",
      "|       2|     1171.3|\n",
      "|       2|     1081.2|\n",
      "|       2|    1059.21|\n",
      "|       2|    1015.55|\n",
      "|       1|     1009.0|\n",
      "|       2|    1004.25|\n",
      "+--------+-----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', col('total_amount').alias('TOTAL_ALIAS')) \\\n",
    "    .sort(['TOTAL_ALIAS', 'VendorID'], ascending=[False, True]).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715d566",
   "metadata": {},
   "source": [
    "There is also the Dataframe.orderBy() method, it's the same implementation of Dataframe.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dedcfff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|    46269.44|\n",
      "|       2|     2246.55|\n",
      "|       2|     1694.31|\n",
      "|       2|      1249.3|\n",
      "|       2|     1240.52|\n",
      "|       2|     1226.25|\n",
      "|       2|      1189.2|\n",
      "|       2|     1183.39|\n",
      "|       2|     1175.52|\n",
      "|       2|      1171.3|\n",
      "|       2|      1081.2|\n",
      "|       2|     1059.21|\n",
      "|       2|     1015.55|\n",
      "|       1|      1009.0|\n",
      "|       2|     1004.25|\n",
      "|       2|     1004.25|\n",
      "|       1|     1003.62|\n",
      "|       2|      1000.0|\n",
      "|       2|      992.06|\n",
      "|       2|      967.09|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).orderBy(df.total_amount, ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3f56e",
   "metadata": {},
   "source": [
    "We can use the Column.desc() method to sort in descending order, without providing the ascending parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "877ab089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|    46269.44|\n",
      "|       2|     2246.55|\n",
      "|       2|     1694.31|\n",
      "|       2|      1249.3|\n",
      "|       2|     1240.52|\n",
      "|       2|     1226.25|\n",
      "|       2|      1189.2|\n",
      "|       2|     1183.39|\n",
      "|       2|     1175.52|\n",
      "|       2|      1171.3|\n",
      "|       2|      1081.2|\n",
      "|       2|     1059.21|\n",
      "|       2|     1015.55|\n",
      "|       1|      1009.0|\n",
      "|       2|     1004.25|\n",
      "+--------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).orderBy(df.total_amount.desc()).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48abc98c",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66cacf1",
   "metadata": {},
   "source": [
    "We can use the DataFrame.filter() or DataFrame.where() methods to filter the data, .where() is an alias for filter(). This methods are used alongside filter operators to create a column expression that represents a `Filter Condition`.\n",
    "\n",
    "Some Filter Operators:\n",
    "\n",
    "| Operation        | Symbol | Example (Expression)                    | Equivalent SQL       |\n",
    "| ---------------- | ------ | --------------------------------------- | -------------------- |\n",
    "| Equal to         | `==`   | `col(\"age\") == 30`                      | `age = 30`           |\n",
    "| Not equal        | `!=`   | `col(\"age\") != 30`                      | `age <> 30`          |\n",
    "| Greater than     | `>`    | `col(\"age\") > 18`                       | `age > 18`           |\n",
    "| Less than        | `<`    | `col(\"age\") < 65`                       | `age < 65`           |\n",
    "| Greater or equal | `>=`   | `col(\"age\") >= 21`                      | `age >= 21`          |\n",
    "| Less or equal    | `<=`   | `col(\"age\") <= 65`                      | `age <= 65`          |\n",
    "| And              | `&`    | `(col(\"age\") > 18) & (col(\"age\") < 65)` | `AND`                |\n",
    "| Or               | `\\|`   | `(col(\"age\") > 18) \\| (col(\"age\") < 65)`| `OR`                 |\n",
    "| Not              | `~`    | `~(col(\"active\") == True)`              | `NOT active = true`  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa634ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       2|      112.89|\n",
      "|       2|      153.85|\n",
      "|       2|      103.86|\n",
      "|       2|      100.13|\n",
      "|       1|      121.75|\n",
      "|       2|      106.97|\n",
      "|       1|      100.39|\n",
      "|       1|      101.89|\n",
      "|       2|      100.13|\n",
      "|       2|      148.26|\n",
      "|       2|      238.89|\n",
      "|       2|      104.21|\n",
      "|       1|      100.09|\n",
      "|       2|      103.86|\n",
      "|       2|      112.75|\n",
      "+--------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').where('total_amount > 100').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "152c8e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       1|        13.8|\n",
      "|       1|        17.2|\n",
      "|       1|        20.8|\n",
      "|       1|       83.44|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').filter(df.VendorID == 1).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f7a824",
   "metadata": {},
   "source": [
    "Multiple Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1c91882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+\n",
      "|VendorID|payment_type|total_amount|\n",
      "+--------+------------+------------+\n",
      "|       2|           2|     1694.31|\n",
      "|       2|           1|     1015.55|\n",
      "|       2|           2|     1059.21|\n",
      "|       2|           1|      1171.3|\n",
      "|       2|           1|      1081.2|\n",
      "|       2|           1|      1189.2|\n",
      "|       2|           1|     1175.52|\n",
      "|       2|           1|     1240.52|\n",
      "|       2|           2|     1183.39|\n",
      "|       2|           1|     1004.25|\n",
      "|       2|           1|     1004.25|\n",
      "|       2|           1|     1226.25|\n",
      "|       2|           1|      1249.3|\n",
      "|       2|           1|     2246.55|\n",
      "+--------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'payment_type', 'total_amount').filter((df.VendorID == 2) & (df.total_amount > 1000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e6735a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------------+\n",
      "|VendorID|payment_type|total_amount|\n",
      "+--------+------------+------------+\n",
      "|       2|           1|     1015.55|\n",
      "|       2|           1|      1171.3|\n",
      "|       2|           1|      1081.2|\n",
      "|       2|           1|      1189.2|\n",
      "|       2|           1|     1175.52|\n",
      "|       2|           1|     1240.52|\n",
      "|       2|           1|     1004.25|\n",
      "|       2|           1|     1004.25|\n",
      "|       2|           1|     1226.25|\n",
      "|       2|           1|      1249.3|\n",
      "|       2|           1|     2246.55|\n",
      "+--------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'payment_type', 'total_amount') \\\n",
    "    .filter((df.VendorID == 2) & (df.total_amount > 1000) & (df.payment_type == 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a165598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       2|     1694.31|\n",
      "|       2|     1015.55|\n",
      "|       2|     1059.21|\n",
      "|       1|    46269.44|\n",
      "|       2|      1171.3|\n",
      "|       2|      1081.2|\n",
      "|       2|      1189.2|\n",
      "|       2|     1175.52|\n",
      "|       2|     1240.52|\n",
      "|       2|     1183.39|\n",
      "|       2|     1004.25|\n",
      "|       2|     1004.25|\n",
      "|       2|     1226.25|\n",
      "|       2|      1249.3|\n",
      "|       2|     2246.55|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount') \\\n",
    "    .filter(((df.VendorID == 2) & (df.total_amount > 1000)) | (df.total_amount > 2000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b414e",
   "metadata": {},
   "source": [
    "The Between operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c59546e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorID|total_amount|\n",
      "+--------+------------+\n",
      "|       1|        15.5|\n",
      "|       2|       15.54|\n",
      "|       2|        15.7|\n",
      "|       2|       15.29|\n",
      "|       2|       15.93|\n",
      "|       1|        15.5|\n",
      "|       1|        15.5|\n",
      "|       2|       15.54|\n",
      "|       1|        15.0|\n",
      "|       2|       15.48|\n",
      "|       2|       15.75|\n",
      "|       2|       15.54|\n",
      "|       2|       15.75|\n",
      "|       1|        15.5|\n",
      "|       2|        15.8|\n",
      "+--------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).where(df.total_amount.between(15, 16)).show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213842d7",
   "metadata": {},
   "source": [
    "Filtering null values can be done with .isnull(), .isNotNull() and .isNaN() methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca307258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+\n",
      "|VendorID|Airport_fee|total_amount|\n",
      "+--------+-----------+------------+\n",
      "|       2|       NULL|        4.76|\n",
      "|       2|       NULL|       18.95|\n",
      "|       2|       NULL|         2.0|\n",
      "|       2|       NULL|        43.4|\n",
      "|       2|       NULL|       39.23|\n",
      "+--------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'Airport_fee', 'total_amount'].where(col('Airport_fee').isNull()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1606d9b",
   "metadata": {},
   "source": [
    "Filter text in a column with .contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce836b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+--------------------+\n",
      "|VendorID|Airport_fee|total_amount|tpep_pickup_datetime|\n",
      "+--------+-----------+------------+--------------------+\n",
      "|       1|       1.75|       57.25| 2025-03-05 00:17:44|\n",
      "|       1|        0.0|       16.35| 2025-03-05 00:02:33|\n",
      "|       2|        0.0|       14.44| 2025-03-05 00:17:56|\n",
      "|       2|        0.0|       19.25| 2025-03-05 00:39:48|\n",
      "|       2|        0.0|       17.16| 2025-03-05 00:20:42|\n",
      "|       2|        0.0|       16.32| 2025-03-05 00:00:03|\n",
      "|       2|        0.0|        30.6| 2025-03-05 00:13:04|\n",
      "|       2|        0.0|        -8.0| 2025-03-05 00:45:09|\n",
      "|       2|        0.0|         8.0| 2025-03-05 00:45:09|\n",
      "|       2|        0.0|       23.94| 2025-03-05 00:22:02|\n",
      "|       2|        0.0|       17.22| 2025-03-05 00:29:21|\n",
      "|       2|       1.75|       71.74| 2025-03-05 00:09:59|\n",
      "|       2|       1.75|      147.72| 2025-03-05 00:26:52|\n",
      "|       1|        0.0|       26.45| 2025-03-05 00:32:22|\n",
      "|       2|        0.0|        16.4| 2025-03-05 00:33:31|\n",
      "|       2|        0.0|       30.19| 2025-03-05 00:32:52|\n",
      "|       2|        0.0|       17.15| 2025-03-05 00:10:36|\n",
      "|       1|        0.0|        25.9| 2025-03-05 00:55:43|\n",
      "|       2|        0.0|       60.83| 2025-03-05 00:34:05|\n",
      "|       2|        0.0|       22.65| 2025-03-05 00:43:58|\n",
      "+--------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'Airport_fee', 'total_amount', 'tpep_pickup_datetime'] \\\n",
    "    .where(col('tpep_pickup_datetime').contains('2025-03-05')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c768adbf",
   "metadata": {},
   "source": [
    "SQL LIKE pattern filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "24400cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+--------------------+\n",
      "|VendorID|Airport_fee|total_amount|tpep_pickup_datetime|\n",
      "+--------+-----------+------------+--------------------+\n",
      "|       2|        0.0|       18.15| 2025-03-01 00:41:51|\n",
      "|       2|        0.0|       38.99| 2025-03-01 00:51:48|\n",
      "|       2|        0.0|       18.06| 2025-03-01 00:51:23|\n",
      "|       2|        0.0|       48.56| 2025-03-01 00:44:51|\n",
      "|       1|        0.0|       26.45| 2025-03-01 00:16:51|\n",
      "|       1|        0.0|       18.55| 2025-03-01 00:56:51|\n",
      "|       1|        0.0|        14.0| 2025-03-01 00:51:04|\n",
      "|       2|        0.0|       21.42| 2025-03-01 00:51:30|\n",
      "|       2|        0.0|       16.38| 2025-03-01 00:51:14|\n",
      "|       2|        0.0|       51.09| 2025-03-01 00:02:51|\n",
      "|       2|        0.0|       14.95| 2025-03-01 00:22:51|\n",
      "|       2|        0.0|       25.62| 2025-03-01 00:03:51|\n",
      "|       2|        0.0|       15.65| 2025-03-01 00:51:35|\n",
      "|       2|        0.0|       26.46| 2025-03-01 00:51:47|\n",
      "|       2|        0.0|       21.75| 2025-03-01 00:50:51|\n",
      "|       2|        0.0|        35.7| 2025-03-01 00:27:51|\n",
      "|       2|        0.0|       16.23| 2025-03-01 00:20:51|\n",
      "|       2|        0.0|       25.62| 2025-03-01 00:51:18|\n",
      "|       2|        0.0|       34.02| 2025-03-01 00:11:51|\n",
      "|       2|        0.0|       15.65| 2025-03-01 00:51:24|\n",
      "+--------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'Airport_fee', 'total_amount', 'tpep_pickup_datetime'] \\\n",
    "    .where(col('tpep_pickup_datetime').like('%51%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36fed5",
   "metadata": {},
   "source": [
    "Case insensitive SQL LIKE pattern filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a14dfdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|store_and_fwd_flag|\n",
      "+--------+------------------+\n",
      "+--------+------------------+\n",
      "\n",
      "+--------+------------------+\n",
      "|VendorID|store_and_fwd_flag|\n",
      "+--------+------------------+\n",
      "|       1|                 N|\n",
      "|       1|                 N|\n",
      "|       2|                 N|\n",
      "|       2|                 N|\n",
      "|       1|                 N|\n",
      "+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.store_and_fwd_flag).where(col('store_and_fwd_flag').like('n')).show(5)\n",
    "df.select(df.VendorID, df.store_and_fwd_flag).where(col('store_and_fwd_flag').ilike('n')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4a8c6",
   "metadata": {},
   "source": [
    "Column.rlike(): SQL LIKE pattern with REGEX filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "197568e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+--------------------+\n",
      "|VendorID|Airport_fee|total_amount|tpep_pickup_datetime|\n",
      "+--------+-----------+------------+--------------------+\n",
      "|       2|        0.0|       18.15| 2025-03-01 00:41:51|\n",
      "|       2|        0.0|       48.56| 2025-03-01 00:44:51|\n",
      "|       1|        0.0|       26.45| 2025-03-01 00:16:51|\n",
      "|       1|        0.0|       18.55| 2025-03-01 00:56:51|\n",
      "|       2|        0.0|       51.09| 2025-03-01 00:02:51|\n",
      "|       2|        0.0|       14.95| 2025-03-01 00:22:51|\n",
      "|       2|        0.0|       25.62| 2025-03-01 00:03:51|\n",
      "|       2|        0.0|       21.75| 2025-03-01 00:50:51|\n",
      "|       2|        0.0|        35.7| 2025-03-01 00:27:51|\n",
      "|       2|        0.0|       16.23| 2025-03-01 00:20:51|\n",
      "|       2|        0.0|       34.02| 2025-03-01 00:11:51|\n",
      "|       1|        0.0|        15.3| 2025-03-01 00:51:51|\n",
      "|       2|        0.0|       15.05| 2025-03-01 00:17:51|\n",
      "|       2|        0.0|        18.9| 2025-03-01 00:35:51|\n",
      "|       1|        0.0|       17.95| 2025-03-01 00:35:51|\n",
      "|       2|        0.0|       18.65| 2025-03-01 00:48:51|\n",
      "|       2|        0.0|        15.7| 2025-03-01 00:24:51|\n",
      "|       1|        0.0|       24.15| 2025-03-01 00:04:51|\n",
      "|       2|        0.0|       36.85| 2025-03-01 00:37:51|\n",
      "|       1|        0.0|       24.05| 2025-03-01 00:09:51|\n",
      "+--------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'Airport_fee', 'total_amount', 'tpep_pickup_datetime'] \\\n",
    "    .where(col('tpep_pickup_datetime').rlike('.51$')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4184c",
   "metadata": {},
   "source": [
    "For advanced Regex operations check: regexp_count(), regexp_extract(), regexp_extract() and more at: https://spark.apache.org/docs/latest/api/python/search.html?q=regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e646a4b",
   "metadata": {},
   "source": [
    "Startswith, filter strings that starts with the specified string filter without using LIKE or REGEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5bfb1b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+--------------------+\n",
      "|VendorID|Airport_fee|total_amount|tpep_pickup_datetime|\n",
      "+--------+-----------+------------+--------------------+\n",
      "|       2|        0.0|        27.3| 2025-03-30 23:59:06|\n",
      "|       2|        0.0|       20.58| 2025-03-30 23:00:05|\n",
      "|       1|       1.75|        71.0| 2025-03-30 23:18:41|\n",
      "|       2|       1.75|       53.05| 2025-03-30 23:55:58|\n",
      "|       1|       1.75|        91.8| 2025-03-30 23:05:01|\n",
      "|       1|        0.0|       12.25| 2025-03-30 23:48:17|\n",
      "|       2|        0.0|       26.46| 2025-03-30 23:00:27|\n",
      "|       2|        0.0|        14.7| 2025-03-30 23:48:30|\n",
      "|       1|        0.0|        19.7| 2025-03-30 23:33:25|\n",
      "|       1|        0.0|       14.35| 2025-03-30 23:07:44|\n",
      "|       1|        0.0|       13.65| 2025-03-30 23:17:18|\n",
      "|       2|        0.0|       24.15| 2025-03-30 23:22:33|\n",
      "|       1|        0.0|        11.5| 2025-03-30 23:04:25|\n",
      "|       2|        0.0|       29.58| 2025-03-30 23:01:48|\n",
      "|       1|        0.0|       17.15| 2025-03-30 23:03:07|\n",
      "|       1|        0.0|        33.5| 2025-03-30 23:59:23|\n",
      "|       2|       1.75|      234.69| 2025-03-30 23:51:00|\n",
      "|       1|       1.75|       76.49| 2025-03-30 23:24:34|\n",
      "|       2|        0.0|       20.65| 2025-03-30 23:07:40|\n",
      "|       2|        0.0|       21.35| 2025-03-30 23:08:41|\n",
      "+--------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['VendorID', 'Airport_fee', 'total_amount', 'tpep_pickup_datetime'] \\\n",
    "    .where(col('tpep_pickup_datetime').startswith('2025-03-30 23')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7338e51",
   "metadata": {},
   "source": [
    "Filter unique rows, just like `select distinct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5aca3baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|VendorID|\n",
      "+--------+\n",
      "|       1|\n",
      "|       7|\n",
      "|       2|\n",
      "|       6|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e507445",
   "metadata": {},
   "source": [
    "## Handling columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfaf2bf",
   "metadata": {},
   "source": [
    "Create columns\n",
    "\n",
    "Dataframe.withColumn() creates a new a column based on an expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90dcf3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+\n",
      "|VendorID|total_amount|new_id|\n",
      "+--------+------------+------+\n",
      "|       1|        15.5|   100|\n",
      "|       1|        13.8|   100|\n",
      "|       2|       25.81|   200|\n",
      "|       2|       15.54|   200|\n",
      "|       1|        17.2|   100|\n",
      "+--------+------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').withColumn('new_id', df.VendorID * 100).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cad1b",
   "metadata": {},
   "source": [
    "We can create multiple columns in a single operation with Dataframe.withColumns(), it receives a dictionary of names and columns, the lit() function creates a column based on a literal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69182414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+------+----------------+\n",
      "|VendorID|total_amount|new_id|new_total_amount|\n",
      "+--------+------------+------+----------------+\n",
      "|       1|        15.5|   100|        $   15.5|\n",
      "|       1|        13.8|   100|        $   13.8|\n",
      "|       2|       25.81|   200|       $   25.81|\n",
      "|       2|       15.54|   200|       $   15.54|\n",
      "|       1|        17.2|   100|        $   17.2|\n",
      "+--------+------------+------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount') \\\n",
    "    .withColumns({\n",
    "        'new_id': df.VendorID * 100, \n",
    "        'new_total_amount': concat(lit('$   '), df.total_amount)\n",
    "        }).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e370b",
   "metadata": {},
   "source": [
    "Column.when(), Functions.when(), Column.otherwise(), Functions.otherwise() are the equivalent to SQL CASE, WHEN and ELSE on pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c38664a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID|    Case Statement|\n",
      "+--------+------------------+\n",
      "|       7|             Helix|\n",
      "|       1|   Creative Mobile|\n",
      "|       2|Curb Mobility, LLC|\n",
      "|       6| Myle Technologies|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', \n",
    "    when(df.VendorID == 1, 'Creative Mobile').when(df.VendorID == 2, 'Curb Mobility, LLC') \\\n",
    "    .when(df.VendorID == 6, 'Myle Technologies').otherwise('Helix') \\\n",
    "    .alias('Case Statement')\n",
    "    ).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f687b00",
   "metadata": {},
   "source": [
    "Return or change a column based on a condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f048c11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|VendorId|total_amount|\n",
      "+--------+------------+\n",
      "|     200|        15.5|\n",
      "|     200|        13.8|\n",
      "|     100|       25.81|\n",
      "|     200|       15.54|\n",
      "|     200|        17.2|\n",
      "|     200|        20.8|\n",
      "|     100|        27.3|\n",
      "|     200|       16.35|\n",
      "|     200|        23.1|\n",
      "|     200|       21.42|\n",
      "|     200|       14.35|\n",
      "|     200|       14.25|\n",
      "|     100|       83.44|\n",
      "|     100|      112.89|\n",
      "|     100|       28.14|\n",
      "|     200|        15.7|\n",
      "|     200|       18.15|\n",
      "|     200|       16.35|\n",
      "|     200|       24.75|\n",
      "|     200|       12.95|\n",
      "+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.VendorID, df.total_amount).withColumn('VendorId',\n",
    "    when(df.total_amount >= 25, 100).when(df.total_amount < 25, 200).otherwise(df.VendorID)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f28f8",
   "metadata": {},
   "source": [
    "Dataframes and column .alias() method return a Dataframe with an **alias set** applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4549ca05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|VendorID|TOTAL_ALIAS|\n",
      "+--------+-----------+\n",
      "|       1|       15.5|\n",
      "|       1|       13.8|\n",
      "|       2|      25.81|\n",
      "|       2|      15.54|\n",
      "|       1|       17.2|\n",
      "+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', col('total_amount').alias('TOTAL_ALIAS')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d2116a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, TOTAL_ALIAS: double]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('VendorID', col('total_amount').alias('TOTAL_ALIAS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3130a242",
   "metadata": {},
   "source": [
    "Renaming Columns is different from giving them aliases because if the column to be renamed doesn't exists, it doesn't change anything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c079ff",
   "metadata": {},
   "source": [
    "Rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c1c4edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| Id|total_amount|\n",
      "+---+------------+\n",
      "|  1|        15.5|\n",
      "|  1|        13.8|\n",
      "|  2|       25.81|\n",
      "|  2|       15.54|\n",
      "|  1|        17.2|\n",
      "+---+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').withColumnRenamed('VendorID', 'Id').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43208ded",
   "metadata": {},
   "source": [
    "Rename multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3c025bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| Id|amount|\n",
      "+---+------+\n",
      "|  1|  15.5|\n",
      "|  1|  13.8|\n",
      "|  2| 25.81|\n",
      "|  2| 15.54|\n",
      "|  1|  17.2|\n",
      "+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').withColumnsRenamed({'VendorID': 'Id', 'total_amount': 'amount'}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d7370",
   "metadata": {},
   "source": [
    "Drop columns: The Dataframe.Drop() method returns a new dataframe without the columns specified in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fb7917d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|VendorID|\n",
      "+--------+\n",
      "|       1|\n",
      "|       1|\n",
      "|       2|\n",
      "|       2|\n",
      "|       1|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').drop('total_amount').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df5982b",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acce831",
   "metadata": {},
   "source": [
    "Group By functions: COUNT, SUM, MIN, MAX, AVG, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccbf02c",
   "metadata": {},
   "source": [
    "Dataframe.GroupBy returns a DataFrameGroupBy class that is used to create aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d2940f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GroupedData[grouping expressions: [VendorID], value: [VendorID: int, tpep_pickup_datetime: timestamp_ntz ... 18 more fields], type: GroupBy]\n"
     ]
    }
   ],
   "source": [
    "print(df.groupBy('VendorID'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff906f",
   "metadata": {},
   "source": [
    "Count Aggregation: counts the rows by the groupby class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db7f28c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|VendorID|  count|\n",
      "+--------+-------+\n",
      "|       1| 834394|\n",
      "|       7|  21481|\n",
      "|       2|3289048|\n",
      "|       6|    334|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('VendorID').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67edcb",
   "metadata": {},
   "source": [
    "Mean, sum, min, max and avg compute the function for all columns by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6cacd692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------------+------------------+-------------------------+-------------------+-----------------------+\n",
      "|VendorID|avg(VendorID)|avg(passenger_count)|avg(trip_distance)|   avg(RatecodeID)| avg(PULocationID)| avg(DOLocationID)| avg(payment_type)|  avg(fare_amount)|        avg(extra)|       avg(mta_tax)|   avg(tip_amount)|  avg(tolls_amount)|avg(improvement_surcharge)| avg(total_amount)|avg(congestion_surcharge)|   avg(Airport_fee)|avg(cbd_congestion_fee)|\n",
      "+--------+-------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------------+------------------+-------------------------+-------------------+-----------------------+\n",
      "|       1|          1.0|  1.1356357489615063| 3.424591979328704| 7.168901724390824|163.15342152508288|162.15397761728872|0.9883723996097766|  18.4322967087491|3.1941489751843832|0.49553396836506497|2.8565741723933864|0.49396246857008635|        0.9520118792800524| 26.81411121125252|        2.206140107288467|0.10320747516088286|     0.5323752927274166|\n",
      "|       7|          7.0|   1.293608305013733| 2.988788696988037|1.0349611284390856| 170.7460546529491|167.22908616917275|1.1509240724361063|15.903308970718298|               0.0|                0.5| 3.557886038825003| 0.3956719891997602|                       1.0|25.361795540244803|        2.400726223173968|0.27089055444346166|     0.5585633815930358|\n",
      "|       2|          2.0|  1.3346868170868706| 7.408857827552657|1.1050775002854412| 161.3580844669947|160.95823867575055|0.9467313946163145|17.653924007798313|0.7284373685029834| 0.4741264037496564|2.8551598000390745| 0.4705445101440829|        0.9581881444113919| 26.13239850861697|       2.2215550837680156|0.14290554809904132|     0.5373645352697802|\n",
      "|       6|          6.0|                NULL| 9.088413173652697|              NULL|126.58982035928143|127.58083832335329|               0.0|2.9173652694610643|               0.0|                0.5|               0.0|   1.78862275449102|        0.2999999999999982|29.504341317365295|                     NULL|               NULL|                    0.0|\n",
      "+--------+-------------+--------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------------+------------------+-------------------------+-------------------+-----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupBy('VendorID').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe1327",
   "metadata": {},
   "source": [
    "The last output showed every column, remember to select first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2956eb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID| avg(total_amount)|\n",
      "+--------+------------------+\n",
      "|       1| 26.81411121125252|\n",
      "|       7|25.361795540244803|\n",
      "|       2| 26.13239850861697|\n",
      "|       6|29.504341317365295|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('VendorID', 'total_amount').groupBy('VendorID').mean('total_amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5918b28",
   "metadata": {},
   "source": [
    "Grouping without .select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c849ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|VendorID| avg(total_amount)|\n",
      "+--------+------------------+\n",
      "|       1| 26.81411121125252|\n",
      "|       7|25.361795540244803|\n",
      "|       2| 26.13239850861697|\n",
      "|       6|29.504341317365295|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('VendorID').mean('total_amount').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51371365",
   "metadata": {},
   "source": [
    "We can compute multiple aggregations across the entire dataframe with the .agg() method, this is also an alias for DataFrame.groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61020a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+---------------+----------+------------------+\n",
      "|min(trip_distance)|sum(tolls_amount)|count(VendorID)|max(extra)| avg(total_amount)|\n",
      "+------------------+-----------------+---------------+----------+------------------+\n",
      "|               0.0|1968899.629999516|        4145257|      13.5|26.265898046844185|\n",
      "+------------------+-----------------+---------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({ 'total_amount': 'avg', 'trip_distance': 'min', 'extra': 'max', \\\n",
    "        'VendorID': 'count', 'tolls_amount': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534e4396",
   "metadata": {},
   "source": [
    "## Join and Union operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29809a80",
   "metadata": {},
   "source": [
    "Spark has **join types**, just like SQL, these are the ways that we can **join** (mix and match) the data between DataFrames(tables). Some examples: Inner, left, right, full outer, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4905f11",
   "metadata": {},
   "source": [
    "Spark also has **Join Srategies** these are the ways or algorithms used to shuffle, mix and match the data across nodes and executors in the spark cluster. Examples: Broadcast, sort merge, shuffle, etc; we can read about these in the [performance tuning documentation](!https://spark.apache.org/docs/latest/sql-performance-tuning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83628f93",
   "metadata": {},
   "source": [
    "We can join dataframes using `DataFrame.join(other, on=None, how=None)`, `inner` is the default value of the `how` parameter, and `other` is the  right side dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4f8907ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------+\n",
      "|VendorID|trip_distance|total_amount|\n",
      "+--------+-------------+------------+\n",
      "|       1|          0.9|        15.5|\n",
      "+--------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare dataframes\n",
    "df_a = df.select(df.VendorID, df.trip_distance).limit(1)\n",
    "df_b = df.select(df.VendorID, df.total_amount).limit(1)\n",
    "df_join = df_a.join(df_b, 'VendorID', 'inner')\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e24347",
   "metadata": {},
   "source": [
    "Union concatenates DataFrames with the same schema, using the column's position, like SQL UNION, but with the caveat that there isn't automatic deduplication of rows, this makes spark's union closer to SQL UNION ALL, the SQL UNION behavior can be archieved with .union().distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02ef5d",
   "metadata": {},
   "source": [
    "In this example both dataframes have integer and double columns, making union possible, the output columns are VendorID and trip_distance because df_a was the main DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e1b24d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|VendorID|trip_distance|\n",
      "+--------+-------------+\n",
      "|       1|          0.9|\n",
      "|       1|         15.5|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union = df_a.union(df_b)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9885de",
   "metadata": {},
   "source": [
    ".unionAll() ALL is an alias to .union() in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7eb58f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal spark Union (SQL Union All): 3\n",
      "Deduplicated spark Union (Union): 2\n"
     ]
    }
   ],
   "source": [
    "print(f'Normal spark Union (SQL Union All): {df_a.unionAll(df_union).count()}')\n",
    "print(f'Deduplicated spark Union (Union): {df_a.unionAll(df_union).distinct().count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd53e31c",
   "metadata": {},
   "source": [
    "# Spark Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8196b",
   "metadata": {},
   "source": [
    "The spark catalog is the metadata store used for every SQL objects in a spark session. By default spark will use the Session Catalog, saved in the spark driver's memory. \n",
    "\n",
    "There are several Catalogs that Spark can utilize such as:\n",
    "- Spark Session Catalog\n",
    "- Hive Catalog\n",
    "- Delta Catalog\n",
    "- Hudi Catalog\n",
    "- Iceberg Catalog and it's variants:\n",
    "    - Hadoop\n",
    "    - Hive\n",
    "    - Rest\n",
    "    - Nessie\n",
    "- Unity Catalog\n",
    "- JDBC Catalog\n",
    "- Custom Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ecd74",
   "metadata": {},
   "source": [
    "let's check some basic catalog operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347fbf5d",
   "metadata": {},
   "source": [
    "List Catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "47632cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CatalogMetadata(name='spark_catalog', description=None)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listCatalogs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8b2a7",
   "metadata": {},
   "source": [
    "Set default catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d7ee4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentCatalog('spark_catalog')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464fc27d",
   "metadata": {},
   "source": [
    "Print the default catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fcb46e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83634ad",
   "metadata": {},
   "source": [
    "List all Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ad3a0e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/mnt/home/repos/learn_pyspark/spark-warehouse')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f9582",
   "metadata": {},
   "source": [
    "Set default database, just like USE in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "48c9ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.setCurrentDatabase('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7924b",
   "metadata": {},
   "source": [
    "Check default database in session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "89c3198f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8f973",
   "metadata": {},
   "source": [
    "Get a database object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "57baad2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/mnt/home/repos/learn_pyspark/spark-warehouse')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.getDatabase('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d79396",
   "metadata": {},
   "source": [
    "Check if a table exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "77bae11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.tableExists('union_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e26f59",
   "metadata": {},
   "source": [
    "Create a table, this table is managed by the spark catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa8e42fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "union_example table created\n"
     ]
    }
   ],
   "source": [
    "if not spark.catalog.tableExists('union_example'):\n",
    "    spark.catalog.createTable('union_example', schema=df_union.schema)\n",
    "    print('union_example table created')\n",
    "else:\n",
    "    print('Table already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf1bcc",
   "metadata": {},
   "source": [
    "Create a table from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "43ab7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.write.format('parquet').mode('overwrite').saveAsTable('df_union_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a027e8",
   "metadata": {},
   "source": [
    "Create an external table(unmanaged tables), these are not managed by the spark catalog and we must specify where are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83a36801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[VendorID: int, trip_distance: double]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root directory of external_tables\n",
    "os.makedirs('spark-warehouse/external_tables/union_external_table', exist_ok=True)\n",
    "spark.catalog.createTable(tableName='union_external_table', \n",
    "    path='external_tables/union_external_table', schema=df_union.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3e27727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.write.format('parquet').mode('overwrite').saveAsTable('external2', \n",
    "    path='external_tables/external2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a3d3a",
   "metadata": {},
   "source": [
    "Get a table object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ca588ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(name='union_example', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.getTable('union_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f99776",
   "metadata": {},
   "source": [
    "Create a temporal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d2e84b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_union.createOrReplaceTempView('union_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6153f4",
   "metadata": {},
   "source": [
    "Drop temporal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "792d6647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.dropTempView('union_view')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7254031",
   "metadata": {},
   "source": [
    "List tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d806fb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='df_union_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='external2', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='union_example', catalog='spark_catalog', namespace=['default'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='union_external_table', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d18db",
   "metadata": {},
   "source": [
    "List columns of a table or view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "209d50d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='VendorID', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='trip_distance', description=None, dataType='double', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns('union_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfdbc60",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11472620",
   "metadata": {},
   "source": [
    "We can execute SQL code in pyspark using spark.sql(), this is the Spark SQL API.\n",
    "\n",
    "Almost everything in this guide can be done using spark sql instead of the dataframe api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "50c1c9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('CREATE DATABASE example_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9f8fd230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|   default|\n",
      "|example_db|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "283e2e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE example_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b64b029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP DATABASE IF EXISTS example_db')\n",
    "spark.sql('SHOW DATABASES').show()\n",
    "spark.sql(\"USE default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0dee09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare example\n",
    "df_union.createOrReplaceTempView('union_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1a3c989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+\n",
      "|VendorID|trip_distance|\n",
      "+--------+-------------+\n",
      "|       1|          0.9|\n",
      "|       1|         15.5|\n",
      "+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM union_view').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6aac1c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "    CREATE OR REPLACE TEMP VIEW union_view2 AS\n",
    "        SELECT *, CONCAT(trip_distance, ' miles') AS new_distance \n",
    "        FROM union_view\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c6efb37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+------------+\n",
      "|VendorID|trip_distance|new_distance|\n",
      "+--------+-------------+------------+\n",
      "|       1|          0.9|   0.9 miles|\n",
      "|       1|         15.5|  15.5 miles|\n",
      "+--------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM union_view2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "15fbc686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|     VendorID|      int|   NULL|\n",
      "|trip_distance|   double|   NULL|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE TABLE union_example').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93bd3113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS union_example')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d421b58",
   "metadata": {},
   "source": [
    "## Explain query Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2201ae73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Union\n",
      "   :- GlobalLimit 1, 0\n",
      "   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2452]\n",
      "   :     +- LocalLimit 1\n",
      "   :        +- FileScan parquet [VendorID#4,trip_distance#8] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/home/repos/learn_pyspark/data/yellow_taxi_data_202503.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,trip_distance:double>\n",
      "   +- GlobalLimit 1, 0\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2454]\n",
      "         +- LocalLimit 1\n",
      "            +- FileScan parquet [VendorID#4304,total_amount#4320] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/home/repos/learn_pyspark/data/yellow_taxi_data_202503.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,total_amount:double>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_union.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6192ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|plan                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|== Physical Plan ==\\nAdaptiveSparkPlan isFinalPlan=false\\n+- Union\\n   :- GlobalLimit 1, 0\\n   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2488]\\n   :     +- LocalLimit 1\\n   :        +- FileScan parquet [VendorID#4,trip_distance#8] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/home/repos/learn_pyspark/data/yellow_taxi_data_202503.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,trip_distance:double>\\n   +- GlobalLimit 1, 0\\n      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=2490]\\n         +- LocalLimit 1\\n            +- FileScan parquet [VendorID#4304,total_amount#4320] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/home/repos/learn_pyspark/data/yellow_taxi_data_202503.parquet], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,total_amount:double>\\n\\n|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('EXPLAIN SELECT * FROM union_view').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dd5b3",
   "metadata": {},
   "source": [
    "Shut down the current SparkSession with SparkSession.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a2779491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
